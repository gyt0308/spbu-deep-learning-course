{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Часть I"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Бинарная классификация в качестве примера рассматриваемых метрик (Precision, recall, F1, accuracy, roc-auc, pr-auc, specificity ......) Какие из этих метрик чувствительны к дисбалансу классов\n",
        "Предположим, у нас есть тестовый набор из 1000 образцов, в котором 10 положительных примеров (P) и 990 отрицательных (N).\n",
        "\n",
        "1, точность (Precision): если модель предсказывает все образцы как отрицательные, то количество положительных случаев, предсказанных TP (True Positive), равно 0, FP (False Positive) равно 0, тогда точность $$ Precision = TP / (TP + FP) = 0. $$\n",
        "Прецизионность - это доля всех образцов, предсказанных как положительные, которые на самом деле являются положительными. В задачах бинарной классификации дисбаланс классов может сильно повлиять на точность, так как количество неверно предсказанных отрицательных примеров напрямую влияет на точность. Если модель предсказывает все образцы как отрицательные, то, хотя модель предсказывает отрицательные примеры с высокой степенью точности, ее точность равна 0, поскольку она не предсказывает правильно ни одного положительного примера.\n",
        "\n",
        "2. recall также очень чувствителен к дисбалансу классов, потому что увеличение количества отрицательных примеров не влияет на recall, но уменьшение количества примеров нескольких классов напрямую приводит к уменьшению recall.\n",
        "В приведенном выше примере, если модель предсказывает все примеры как отрицательные, то количество предсказанных положительных примеров, TP, равно 0, поэтому $$ Recall = TP/(TP+FN) = 0 $$\n",
        "\n",
        "3, F1 значение: F1 значение является точность и recall примирения среднего, и обратить больше внимания на баланс двух. Значение F1 менее чувствительно к дисбалансу классов, чем точность и recall.\n",
        "В приведенном выше примере $$F1 = 2PrecisionRecall / (Precision + Recall) = 2 * 0 = 0. $$ Так как точность и recall равны 0, то и значение F1 также равно 0.\n",
        "\n",
        "4, Точность (Accuracy): Точность - это доля всех правильно предсказанных образцов к общему числу образцов. Точность не является хорошим показателем эффективности модели, если в данных наблюдается сильный дисбаланс категорий. В крайних случаях точность может быть высокой, даже если модель предсказывает только основные категории. Если модель предсказывает все образцы как отрицательные, то количество образцов, правильно предсказанных как TN (True Negative, действительно отрицательные и предсказанные как отрицательные), равно 990, а количество образцов, неправильно предсказанных как FP, равно 0. Полученная точность рассчитывается как $$ Accuracy = (TP+TN)/(P+N) = 990/1000 = 99 $$ процентов\n",
        "\n",
        "5, ROC-AUC: площадь ROC-кривой (AUC), метрика оценки, используемая для решения задач бинарной классификации, относительно нечувствительна к дисбалансу категорий.\n",
        "\n",
        "6、PR-AUC：Площадь кривой PR (AUC), является своего рода оценочным показателем, используемым для решения задач бинарной классификации. По сравнению с ROC-AUC, PR-AUC более чувствителен к дисбалансу категорий.\n",
        "\n",
        "7, Специфичность (Specificity): специфичность - это доля всех реальных отрицательных случаев, которые правильно предсказаны как отрицательные, и она также относительно чувствительна к дисбалансу категорий. В случае дисбаланса классов неправильное предсказание нескольких классов приводит к снижению специфичности.\n",
        "В приведенном выше примере специфичность равна 100 %, поскольку все отрицательные примеры были предсказаны правильно.\n",
        "\n",
        "В этом примере модель предсказывает отрицательные примеры для всех образцов и не получает никакой ценной информации, но ее точность и специфичность высоки, что показывает, что точность и специфичность очень чувствительны к дисбалансу категорий. В целом, показатели Accuracy, Precision, Recall и Specificity чувствительны к дисбалансу классов, в то время как F1 score, ROC-AUC более стабильны, а PR-AUC наиболее чувствителен к дисбалансу классов."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 2.Напишите формулу для F_beta и объясните влияние параметра beta, приведя пример того, что происходит, когда beta больше 1.\n",
        "$$ F_beta = (1+beta^2) * (Precision * Recall) / (beta^2 * Precision + Recall) $$\n",
        "\n",
        "где beta - неотрицательный параметр, который регулирует вес между Precision и Recall. Значение beta влияет на то, сколько внимания в оценке F_beta уделяется Precision и Recall.\n",
        "\n",
        "Когда beta=1, Precision и Recall имеют одинаковый вес, что часто используется для оценки F1.\n",
        "Когда beta > 1, recall имеет большее влияние, чем precision. Это происходит потому, что в формуле бета^2 увеличивает вес отзыва. \n",
        "Когда бета<1, точность будет иметь большее влияние, чем отзыв."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 3.Приведите пример расчета ROC-AUC не менее чем для 6 предсказаний.\n",
        "Предположим, у нас есть следующие данные:\n",
        "\n",
        "|Истинное значение |Прогнозируемая вероятность|\n",
        "|------------------|--------------------------|\n",
        "|1 |0.9|\n",
        "|0 |0.8|\n",
        "|1 |0.7|\n",
        "|1 |0.6|\n",
        "|0 |0.55|\n",
        "|1 |0.52|\n",
        "\n",
        "TPR и FPR были рассчитаны для каждого порога в порядке убывания предсказанной вероятности.\n",
        "\n",
        "| Порог| TPR |FPR|\n",
        "|------|-----|---|\n",
        "|0.9   |0/4  |0/2|\n",
        "|0.8   |1/4  |1/2|\n",
        "|0.7   |2/4  |1/2|\n",
        "|0.6   |3/4  |1/2|\n",
        "|0.55  |4/4  |2/2|\n",
        "|0.52  |4/4  |2/2|\n",
        "\n",
        "AUC - площадь под ROC-кривой - может быть рассчитана следующим образом.\n",
        "\n",
        "$$ AUC = 0,5*(1/2)(1/4) + 1(1/4) + 1*(1/4) + 0,5*(1/2)*(1/4) = 0,5$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 4. Сравнение микронаборов точности и отзыва со взвешенными наборами (с примерами)\n",
        "Precision и Recall - важные метрики для оценки эффективности моделей классификации, которые широко используются в задачах бинарной и мультиклассификации. Перекрестная комбинация Precision и Recall позволяет получить F-Score для комплексной оценки эффективности модели.\n",
        "\n",
        "Микро-среднее и макро-среднее - вычислительные стратегии в оценке многокатегориальных задач, которые в основном используются для решения проблем дисбаланса категорий.\n",
        "\n",
        "Микро-среднее: TP (истинные случаи), FP (ложноположительные случаи) и FN (ложноотрицательные случаи) всех категорий объединяются, а затем рассчитываются точность и Recall.\n",
        "Макро-среднее: вычисляются Precision и Recall для каждой категории отдельно, а затем берется среднее значение.\n",
        "Вот пример:\n",
        "\n",
        "Предположим, есть три категории A, B и C со следующими матрицами путаницы:\n",
        "\n",
        "|категории  |матрицами путаницы   |\n",
        "|-----------|---------------------|\n",
        "|Категория A| TP=100, FP=50, FN=30|\n",
        "|Категория B| TP=200, FP=60, FN=40|\n",
        "|Категория C| TP=150, FP=40, FN=50|\n",
        "\n",
        "Расчет микроусреднения:\n",
        "\n",
        "$$ Precision (Precision_micro) = (100+200+150) / (100+200+150 + 50+60+40) = 78,125%.$$\n",
        "$$ Recall (Recall_micro) = (100+200+150) / (100+200+150 + 30+40+50) = 81.355% $$\n",
        "Рассчитывается среднее значение для макросов:\n",
        "\n",
        "$$ Precision (Precision_macro) = [(100/150) + (200/260) + (150/190)] / 3 = 77,399% $$\n",
        "$$ Recall (Recall_macro) = [(100 / 130) + (200 / 240) + (150/200)] / 3 = 77,778% $$\n",
        "Как видно, микроусреднение - это глобальная метрика оценки, которая рассматривает все категории как одинаково важные, в то время как макроусреднение - это локализованная метрика оценки, которая рассматривает каждую категорию как отдельную дихотомическую задачу и подходит для случая дисбаланса категорий."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Часть II"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zV3CWmfZ3ZM4",
        "outputId": "5308c81e-c2ad-4347-e205-95493c55de31"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting onnx\n",
            "  Downloading onnx-1.15.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (15.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.7/15.7 MB\u001b[0m \u001b[31m32.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting onnxruntime\n",
            "  Downloading onnxruntime-1.16.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.4/6.4 MB\u001b[0m \u001b[31m28.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tf2onnx\n",
            "  Downloading tf2onnx-1.15.1-py3-none-any.whl (454 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m454.7/454.7 kB\u001b[0m \u001b[31m43.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from onnx) (1.23.5)\n",
            "Requirement already satisfied: protobuf>=3.20.2 in /usr/local/lib/python3.10/dist-packages (from onnx) (3.20.3)\n",
            "Collecting coloredlogs (from onnxruntime)\n",
            "  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: flatbuffers in /usr/local/lib/python3.10/dist-packages (from onnxruntime) (23.5.26)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from onnxruntime) (23.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from onnxruntime) (1.12)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from tf2onnx) (2.31.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from tf2onnx) (1.16.0)\n",
            "Collecting humanfriendly>=9.1 (from coloredlogs->onnxruntime)\n",
            "  Downloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->tf2onnx) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->tf2onnx) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->tf2onnx) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->tf2onnx) (2023.11.17)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->onnxruntime) (1.3.0)\n",
            "Installing collected packages: onnx, humanfriendly, tf2onnx, coloredlogs, onnxruntime\n",
            "Successfully installed coloredlogs-15.0.1 humanfriendly-10.0 onnx-1.15.0 onnxruntime-1.16.3 tf2onnx-1.15.1\n"
          ]
        }
      ],
      "source": [
        "!pip install onnx onnxruntime tf2onnx"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xd-zAm5DrKI7"
      },
      "source": [
        "# Методы оптимизации моделей"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6Y8vf2db9PCU",
        "outputId": "33f28dee-f456-4c4c-a020-b41774eacc53"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ./MNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 9912422/9912422 [00:00<00:00, 134650760.10it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting ./MNIST/raw/train-images-idx3-ubyte.gz to ./MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ./MNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 28881/28881 [00:00<00:00, 130957506.84it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting ./MNIST/raw/train-labels-idx1-ubyte.gz to ./MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ./MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1648877/1648877 [00:00<00:00, 34973963.29it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting ./MNIST/raw/t10k-images-idx3-ubyte.gz to ./MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ./MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 4542/4542 [00:00<00:00, 18388541.28it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting ./MNIST/raw/t10k-labels-idx1-ubyte.gz to ./MNIST/raw\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "import torchvision\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import accuracy_score\n",
        "import time\n",
        "import onnx\n",
        "import onnxruntime\n",
        "\n",
        "# Load MNIST dataset\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.1307,), (0.3081,))\n",
        "])\n",
        "\n",
        "train_dataset = datasets.MNIST('./', train=True, download=True, transform=transform)\n",
        "test_dataset = datasets.MNIST('./', train=False, download=True, transform=transform)\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "# Split the data into training and validation sets\n",
        "x_train, x_val, y_train, y_val = train_test_split(train_dataset.data.numpy(), train_dataset.targets.numpy(), test_size=0.2, random_state=42)\n",
        "\n",
        "# Reshape data to fit the ImageDataGenerator\n",
        "x_train = x_train.reshape(-1, 28, 28, 1)\n",
        "x_val = x_val.reshape(-1, 28, 28, 1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2WIG2f0SrVxU"
      },
      "source": [
        "# Методы оптимизации моделей"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dGvds2sA9hUY",
        "outputId": "3c9d196f-e666-464b-9a65-ba123ca0b49e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1/20], Batch [100/1500], Loss: 2.4405, Correct predictions: 329, Total samples: 3200, Accuracy: 0.1028\n",
            "Epoch [1/20], Batch [200/1500], Loss: 2.4071, Correct predictions: 386, Total samples: 3200, Accuracy: 0.1206\n",
            "Epoch [1/20], Batch [300/1500], Loss: 2.3863, Correct predictions: 387, Total samples: 3200, Accuracy: 0.1209\n",
            "Epoch [1/20], Batch [400/1500], Loss: 2.3367, Correct predictions: 460, Total samples: 3200, Accuracy: 0.1437\n",
            "Epoch [1/20], Batch [500/1500], Loss: 2.3054, Correct predictions: 494, Total samples: 3200, Accuracy: 0.1544\n",
            "Epoch [1/20], Batch [600/1500], Loss: 2.2872, Correct predictions: 521, Total samples: 3200, Accuracy: 0.1628\n",
            "Epoch [1/20], Batch [700/1500], Loss: 2.2730, Correct predictions: 522, Total samples: 3200, Accuracy: 0.1631\n",
            "Epoch [1/20], Batch [800/1500], Loss: 2.2444, Correct predictions: 576, Total samples: 3200, Accuracy: 0.1800\n",
            "Epoch [1/20], Batch [900/1500], Loss: 2.2087, Correct predictions: 690, Total samples: 3200, Accuracy: 0.2156\n",
            "Epoch [1/20], Batch [1000/1500], Loss: 2.1884, Correct predictions: 682, Total samples: 3200, Accuracy: 0.2131\n",
            "Epoch [1/20], Batch [1100/1500], Loss: 2.1526, Correct predictions: 730, Total samples: 3200, Accuracy: 0.2281\n",
            "Epoch [1/20], Batch [1200/1500], Loss: 2.1425, Correct predictions: 789, Total samples: 3200, Accuracy: 0.2466\n",
            "Epoch [1/20], Batch [1300/1500], Loss: 2.1080, Correct predictions: 853, Total samples: 3200, Accuracy: 0.2666\n",
            "Epoch [1/20], Batch [1400/1500], Loss: 2.0664, Correct predictions: 955, Total samples: 3200, Accuracy: 0.2984\n",
            "Epoch [1/20], Batch [1500/1500], Loss: 2.0460, Correct predictions: 961, Total samples: 3200, Accuracy: 0.3003\n",
            "Epoch [2/20], Batch [100/1500], Loss: 2.0224, Correct predictions: 1012, Total samples: 3200, Accuracy: 0.3162\n",
            "Epoch [2/20], Batch [200/1500], Loss: 1.9970, Correct predictions: 1062, Total samples: 3200, Accuracy: 0.3319\n",
            "Epoch [2/20], Batch [300/1500], Loss: 1.9747, Correct predictions: 1150, Total samples: 3200, Accuracy: 0.3594\n",
            "Epoch [2/20], Batch [400/1500], Loss: 1.9545, Correct predictions: 1160, Total samples: 3200, Accuracy: 0.3625\n",
            "Epoch [2/20], Batch [500/1500], Loss: 1.9204, Correct predictions: 1265, Total samples: 3200, Accuracy: 0.3953\n",
            "Epoch [2/20], Batch [600/1500], Loss: 1.9133, Correct predictions: 1228, Total samples: 3200, Accuracy: 0.3837\n",
            "Epoch [2/20], Batch [700/1500], Loss: 1.8896, Correct predictions: 1289, Total samples: 3200, Accuracy: 0.4028\n",
            "Epoch [2/20], Batch [800/1500], Loss: 1.8727, Correct predictions: 1312, Total samples: 3200, Accuracy: 0.4100\n",
            "Epoch [2/20], Batch [900/1500], Loss: 1.8473, Correct predictions: 1382, Total samples: 3200, Accuracy: 0.4319\n",
            "Epoch [2/20], Batch [1000/1500], Loss: 1.8348, Correct predictions: 1411, Total samples: 3200, Accuracy: 0.4409\n",
            "Epoch [2/20], Batch [1100/1500], Loss: 1.7984, Correct predictions: 1461, Total samples: 3200, Accuracy: 0.4566\n",
            "Epoch [2/20], Batch [1200/1500], Loss: 1.8046, Correct predictions: 1429, Total samples: 3200, Accuracy: 0.4466\n",
            "Epoch [2/20], Batch [1300/1500], Loss: 1.7576, Correct predictions: 1552, Total samples: 3200, Accuracy: 0.4850\n",
            "Epoch [2/20], Batch [1400/1500], Loss: 1.7439, Correct predictions: 1548, Total samples: 3200, Accuracy: 0.4838\n",
            "Epoch [2/20], Batch [1500/1500], Loss: 1.7332, Correct predictions: 1605, Total samples: 3200, Accuracy: 0.5016\n",
            "Epoch [3/20], Batch [100/1500], Loss: 1.7061, Correct predictions: 1623, Total samples: 3200, Accuracy: 0.5072\n",
            "Epoch [3/20], Batch [200/1500], Loss: 1.6855, Correct predictions: 1643, Total samples: 3200, Accuracy: 0.5134\n",
            "Epoch [3/20], Batch [300/1500], Loss: 1.6505, Correct predictions: 1694, Total samples: 3200, Accuracy: 0.5294\n",
            "Epoch [3/20], Batch [400/1500], Loss: 1.6602, Correct predictions: 1719, Total samples: 3200, Accuracy: 0.5372\n",
            "Epoch [3/20], Batch [500/1500], Loss: 1.6440, Correct predictions: 1725, Total samples: 3200, Accuracy: 0.5391\n",
            "Epoch [3/20], Batch [600/1500], Loss: 1.6028, Correct predictions: 1819, Total samples: 3200, Accuracy: 0.5684\n",
            "Epoch [3/20], Batch [700/1500], Loss: 1.6105, Correct predictions: 1779, Total samples: 3200, Accuracy: 0.5559\n",
            "Epoch [3/20], Batch [800/1500], Loss: 1.6103, Correct predictions: 1797, Total samples: 3200, Accuracy: 0.5616\n",
            "Epoch [3/20], Batch [900/1500], Loss: 1.5818, Correct predictions: 1810, Total samples: 3200, Accuracy: 0.5656\n",
            "Epoch [3/20], Batch [1000/1500], Loss: 1.5493, Correct predictions: 1892, Total samples: 3200, Accuracy: 0.5913\n",
            "Epoch [3/20], Batch [1100/1500], Loss: 1.5591, Correct predictions: 1839, Total samples: 3200, Accuracy: 0.5747\n",
            "Epoch [3/20], Batch [1200/1500], Loss: 1.5250, Correct predictions: 1932, Total samples: 3200, Accuracy: 0.6038\n",
            "Epoch [3/20], Batch [1300/1500], Loss: 1.5241, Correct predictions: 1920, Total samples: 3200, Accuracy: 0.6000\n",
            "Epoch [3/20], Batch [1400/1500], Loss: 1.5099, Correct predictions: 1917, Total samples: 3200, Accuracy: 0.5991\n",
            "Epoch [3/20], Batch [1500/1500], Loss: 1.4904, Correct predictions: 1945, Total samples: 3200, Accuracy: 0.6078\n",
            "Epoch [4/20], Batch [100/1500], Loss: 1.4687, Correct predictions: 1962, Total samples: 3200, Accuracy: 0.6131\n",
            "Epoch [4/20], Batch [200/1500], Loss: 1.4571, Correct predictions: 2009, Total samples: 3200, Accuracy: 0.6278\n",
            "Epoch [4/20], Batch [300/1500], Loss: 1.4661, Correct predictions: 1973, Total samples: 3200, Accuracy: 0.6166\n",
            "Epoch [4/20], Batch [400/1500], Loss: 1.4406, Correct predictions: 2021, Total samples: 3200, Accuracy: 0.6316\n",
            "Epoch [4/20], Batch [500/1500], Loss: 1.4371, Correct predictions: 2004, Total samples: 3200, Accuracy: 0.6262\n",
            "Epoch [4/20], Batch [600/1500], Loss: 1.4127, Correct predictions: 2023, Total samples: 3200, Accuracy: 0.6322\n",
            "Epoch [4/20], Batch [700/1500], Loss: 1.3813, Correct predictions: 2125, Total samples: 3200, Accuracy: 0.6641\n",
            "Epoch [4/20], Batch [800/1500], Loss: 1.3897, Correct predictions: 2053, Total samples: 3200, Accuracy: 0.6416\n",
            "Epoch [4/20], Batch [900/1500], Loss: 1.3762, Correct predictions: 2142, Total samples: 3200, Accuracy: 0.6694\n",
            "Epoch [4/20], Batch [1000/1500], Loss: 1.3662, Correct predictions: 2066, Total samples: 3200, Accuracy: 0.6456\n",
            "Epoch [4/20], Batch [1100/1500], Loss: 1.3460, Correct predictions: 2128, Total samples: 3200, Accuracy: 0.6650\n",
            "Epoch [4/20], Batch [1200/1500], Loss: 1.3342, Correct predictions: 2144, Total samples: 3200, Accuracy: 0.6700\n",
            "Epoch [4/20], Batch [1300/1500], Loss: 1.3303, Correct predictions: 2122, Total samples: 3200, Accuracy: 0.6631\n",
            "Epoch [4/20], Batch [1400/1500], Loss: 1.3143, Correct predictions: 2201, Total samples: 3200, Accuracy: 0.6878\n",
            "Epoch [4/20], Batch [1500/1500], Loss: 1.3017, Correct predictions: 2165, Total samples: 3200, Accuracy: 0.6766\n",
            "Epoch [5/20], Batch [100/1500], Loss: 1.2861, Correct predictions: 2204, Total samples: 3200, Accuracy: 0.6887\n",
            "Epoch [5/20], Batch [200/1500], Loss: 1.2860, Correct predictions: 2227, Total samples: 3200, Accuracy: 0.6959\n",
            "Epoch [5/20], Batch [300/1500], Loss: 1.2982, Correct predictions: 2126, Total samples: 3200, Accuracy: 0.6644\n",
            "Epoch [5/20], Batch [400/1500], Loss: 1.2825, Correct predictions: 2214, Total samples: 3200, Accuracy: 0.6919\n",
            "Epoch [5/20], Batch [500/1500], Loss: 1.2559, Correct predictions: 2220, Total samples: 3200, Accuracy: 0.6937\n",
            "Epoch [5/20], Batch [600/1500], Loss: 1.2504, Correct predictions: 2234, Total samples: 3200, Accuracy: 0.6981\n",
            "Epoch [5/20], Batch [700/1500], Loss: 1.2423, Correct predictions: 2225, Total samples: 3200, Accuracy: 0.6953\n",
            "Epoch [5/20], Batch [800/1500], Loss: 1.2365, Correct predictions: 2250, Total samples: 3200, Accuracy: 0.7031\n",
            "Epoch [5/20], Batch [900/1500], Loss: 1.2227, Correct predictions: 2285, Total samples: 3200, Accuracy: 0.7141\n",
            "Epoch [5/20], Batch [1000/1500], Loss: 1.2009, Correct predictions: 2300, Total samples: 3200, Accuracy: 0.7188\n",
            "Epoch [5/20], Batch [1100/1500], Loss: 1.2018, Correct predictions: 2272, Total samples: 3200, Accuracy: 0.7100\n",
            "Epoch [5/20], Batch [1200/1500], Loss: 1.1885, Correct predictions: 2303, Total samples: 3200, Accuracy: 0.7197\n",
            "Epoch [5/20], Batch [1300/1500], Loss: 1.1857, Correct predictions: 2294, Total samples: 3200, Accuracy: 0.7169\n",
            "Epoch [5/20], Batch [1400/1500], Loss: 1.1774, Correct predictions: 2292, Total samples: 3200, Accuracy: 0.7163\n",
            "Epoch [5/20], Batch [1500/1500], Loss: 1.1777, Correct predictions: 2310, Total samples: 3200, Accuracy: 0.7219\n",
            "Epoch [6/20], Batch [100/1500], Loss: 1.1576, Correct predictions: 2355, Total samples: 3200, Accuracy: 0.7359\n",
            "Epoch [6/20], Batch [200/1500], Loss: 1.1766, Correct predictions: 2301, Total samples: 3200, Accuracy: 0.7191\n",
            "Epoch [6/20], Batch [300/1500], Loss: 1.1410, Correct predictions: 2343, Total samples: 3200, Accuracy: 0.7322\n",
            "Epoch [6/20], Batch [400/1500], Loss: 1.1360, Correct predictions: 2343, Total samples: 3200, Accuracy: 0.7322\n",
            "Epoch [6/20], Batch [500/1500], Loss: 1.1451, Correct predictions: 2315, Total samples: 3200, Accuracy: 0.7234\n",
            "Epoch [6/20], Batch [600/1500], Loss: 1.1362, Correct predictions: 2339, Total samples: 3200, Accuracy: 0.7309\n",
            "Epoch [6/20], Batch [700/1500], Loss: 1.1073, Correct predictions: 2404, Total samples: 3200, Accuracy: 0.7512\n",
            "Epoch [6/20], Batch [800/1500], Loss: 1.1238, Correct predictions: 2364, Total samples: 3200, Accuracy: 0.7388\n",
            "Epoch [6/20], Batch [900/1500], Loss: 1.1088, Correct predictions: 2358, Total samples: 3200, Accuracy: 0.7369\n",
            "Epoch [6/20], Batch [1000/1500], Loss: 1.1157, Correct predictions: 2342, Total samples: 3200, Accuracy: 0.7319\n",
            "Epoch [6/20], Batch [1100/1500], Loss: 1.0925, Correct predictions: 2392, Total samples: 3200, Accuracy: 0.7475\n",
            "Epoch [6/20], Batch [1200/1500], Loss: 1.0971, Correct predictions: 2367, Total samples: 3200, Accuracy: 0.7397\n",
            "Epoch [6/20], Batch [1300/1500], Loss: 1.0574, Correct predictions: 2446, Total samples: 3200, Accuracy: 0.7644\n",
            "Epoch [6/20], Batch [1400/1500], Loss: 1.0677, Correct predictions: 2427, Total samples: 3200, Accuracy: 0.7584\n",
            "Epoch [6/20], Batch [1500/1500], Loss: 1.0550, Correct predictions: 2444, Total samples: 3200, Accuracy: 0.7638\n",
            "Epoch [7/20], Batch [100/1500], Loss: 1.0801, Correct predictions: 2389, Total samples: 3200, Accuracy: 0.7466\n",
            "Epoch [7/20], Batch [200/1500], Loss: 1.0732, Correct predictions: 2397, Total samples: 3200, Accuracy: 0.7491\n",
            "Epoch [7/20], Batch [300/1500], Loss: 1.0416, Correct predictions: 2435, Total samples: 3200, Accuracy: 0.7609\n",
            "Epoch [7/20], Batch [400/1500], Loss: 1.0333, Correct predictions: 2444, Total samples: 3200, Accuracy: 0.7638\n",
            "Epoch [7/20], Batch [500/1500], Loss: 1.0513, Correct predictions: 2406, Total samples: 3200, Accuracy: 0.7519\n",
            "Epoch [7/20], Batch [600/1500], Loss: 1.0317, Correct predictions: 2425, Total samples: 3200, Accuracy: 0.7578\n",
            "Epoch [7/20], Batch [700/1500], Loss: 1.0130, Correct predictions: 2467, Total samples: 3200, Accuracy: 0.7709\n",
            "Epoch [7/20], Batch [800/1500], Loss: 1.0179, Correct predictions: 2464, Total samples: 3200, Accuracy: 0.7700\n",
            "Epoch [7/20], Batch [900/1500], Loss: 1.0229, Correct predictions: 2446, Total samples: 3200, Accuracy: 0.7644\n",
            "Epoch [7/20], Batch [1000/1500], Loss: 0.9999, Correct predictions: 2492, Total samples: 3200, Accuracy: 0.7788\n",
            "Epoch [7/20], Batch [1100/1500], Loss: 1.0135, Correct predictions: 2449, Total samples: 3200, Accuracy: 0.7653\n",
            "Epoch [7/20], Batch [1200/1500], Loss: 1.0164, Correct predictions: 2446, Total samples: 3200, Accuracy: 0.7644\n",
            "Epoch [7/20], Batch [1300/1500], Loss: 0.9903, Correct predictions: 2514, Total samples: 3200, Accuracy: 0.7856\n",
            "Epoch [7/20], Batch [1400/1500], Loss: 0.9953, Correct predictions: 2471, Total samples: 3200, Accuracy: 0.7722\n",
            "Epoch [7/20], Batch [1500/1500], Loss: 0.9711, Correct predictions: 2529, Total samples: 3200, Accuracy: 0.7903\n",
            "Epoch [8/20], Batch [100/1500], Loss: 0.9704, Correct predictions: 2491, Total samples: 3200, Accuracy: 0.7784\n",
            "Epoch [8/20], Batch [200/1500], Loss: 0.9647, Correct predictions: 2485, Total samples: 3200, Accuracy: 0.7766\n",
            "Epoch [8/20], Batch [300/1500], Loss: 0.9637, Correct predictions: 2508, Total samples: 3200, Accuracy: 0.7837\n",
            "Epoch [8/20], Batch [400/1500], Loss: 0.9693, Correct predictions: 2534, Total samples: 3200, Accuracy: 0.7919\n",
            "Epoch [8/20], Batch [500/1500], Loss: 0.9553, Correct predictions: 2532, Total samples: 3200, Accuracy: 0.7913\n",
            "Epoch [8/20], Batch [600/1500], Loss: 0.9396, Correct predictions: 2523, Total samples: 3200, Accuracy: 0.7884\n",
            "Epoch [8/20], Batch [700/1500], Loss: 0.9426, Correct predictions: 2556, Total samples: 3200, Accuracy: 0.7987\n",
            "Epoch [8/20], Batch [800/1500], Loss: 0.9469, Correct predictions: 2498, Total samples: 3200, Accuracy: 0.7806\n",
            "Epoch [8/20], Batch [900/1500], Loss: 0.9485, Correct predictions: 2495, Total samples: 3200, Accuracy: 0.7797\n",
            "Epoch [8/20], Batch [1000/1500], Loss: 0.9584, Correct predictions: 2526, Total samples: 3200, Accuracy: 0.7894\n",
            "Epoch [8/20], Batch [1100/1500], Loss: 0.9390, Correct predictions: 2515, Total samples: 3200, Accuracy: 0.7859\n",
            "Epoch [8/20], Batch [1200/1500], Loss: 0.9271, Correct predictions: 2525, Total samples: 3200, Accuracy: 0.7891\n",
            "Epoch [8/20], Batch [1300/1500], Loss: 0.9266, Correct predictions: 2544, Total samples: 3200, Accuracy: 0.7950\n",
            "Epoch [8/20], Batch [1400/1500], Loss: 0.9315, Correct predictions: 2528, Total samples: 3200, Accuracy: 0.7900\n",
            "Epoch [8/20], Batch [1500/1500], Loss: 0.9386, Correct predictions: 2531, Total samples: 3200, Accuracy: 0.7909\n",
            "Epoch [9/20], Batch [100/1500], Loss: 0.8916, Correct predictions: 2589, Total samples: 3200, Accuracy: 0.8091\n",
            "Epoch [9/20], Batch [200/1500], Loss: 0.9290, Correct predictions: 2515, Total samples: 3200, Accuracy: 0.7859\n",
            "Epoch [9/20], Batch [300/1500], Loss: 0.9079, Correct predictions: 2530, Total samples: 3200, Accuracy: 0.7906\n",
            "Epoch [9/20], Batch [400/1500], Loss: 0.9161, Correct predictions: 2535, Total samples: 3200, Accuracy: 0.7922\n",
            "Epoch [9/20], Batch [500/1500], Loss: 0.8941, Correct predictions: 2576, Total samples: 3200, Accuracy: 0.8050\n",
            "Epoch [9/20], Batch [600/1500], Loss: 0.8832, Correct predictions: 2589, Total samples: 3200, Accuracy: 0.8091\n",
            "Epoch [9/20], Batch [700/1500], Loss: 0.8971, Correct predictions: 2562, Total samples: 3200, Accuracy: 0.8006\n",
            "Epoch [9/20], Batch [800/1500], Loss: 0.8988, Correct predictions: 2547, Total samples: 3200, Accuracy: 0.7959\n",
            "Epoch [9/20], Batch [900/1500], Loss: 0.8856, Correct predictions: 2574, Total samples: 3200, Accuracy: 0.8044\n",
            "Epoch [9/20], Batch [1000/1500], Loss: 0.8789, Correct predictions: 2566, Total samples: 3200, Accuracy: 0.8019\n",
            "Epoch [9/20], Batch [1100/1500], Loss: 0.8647, Correct predictions: 2616, Total samples: 3200, Accuracy: 0.8175\n",
            "Epoch [9/20], Batch [1200/1500], Loss: 0.8867, Correct predictions: 2554, Total samples: 3200, Accuracy: 0.7981\n",
            "Epoch [9/20], Batch [1300/1500], Loss: 0.8814, Correct predictions: 2576, Total samples: 3200, Accuracy: 0.8050\n",
            "Epoch [9/20], Batch [1400/1500], Loss: 0.8336, Correct predictions: 2625, Total samples: 3200, Accuracy: 0.8203\n",
            "Epoch [9/20], Batch [1500/1500], Loss: 0.8664, Correct predictions: 2547, Total samples: 3200, Accuracy: 0.7959\n",
            "Epoch [10/20], Batch [100/1500], Loss: 0.8411, Correct predictions: 2614, Total samples: 3200, Accuracy: 0.8169\n",
            "Epoch [10/20], Batch [200/1500], Loss: 0.8666, Correct predictions: 2592, Total samples: 3200, Accuracy: 0.8100\n",
            "Epoch [10/20], Batch [300/1500], Loss: 0.8494, Correct predictions: 2622, Total samples: 3200, Accuracy: 0.8194\n",
            "Epoch [10/20], Batch [400/1500], Loss: 0.8677, Correct predictions: 2604, Total samples: 3200, Accuracy: 0.8137\n",
            "Epoch [10/20], Batch [500/1500], Loss: 0.8516, Correct predictions: 2605, Total samples: 3200, Accuracy: 0.8141\n",
            "Epoch [10/20], Batch [600/1500], Loss: 0.8307, Correct predictions: 2635, Total samples: 3200, Accuracy: 0.8234\n",
            "Epoch [10/20], Batch [700/1500], Loss: 0.8550, Correct predictions: 2588, Total samples: 3200, Accuracy: 0.8087\n",
            "Epoch [10/20], Batch [800/1500], Loss: 0.8340, Correct predictions: 2606, Total samples: 3200, Accuracy: 0.8144\n",
            "Epoch [10/20], Batch [900/1500], Loss: 0.8065, Correct predictions: 2670, Total samples: 3200, Accuracy: 0.8344\n",
            "Epoch [10/20], Batch [1000/1500], Loss: 0.8558, Correct predictions: 2558, Total samples: 3200, Accuracy: 0.7994\n",
            "Epoch [10/20], Batch [1100/1500], Loss: 0.8115, Correct predictions: 2648, Total samples: 3200, Accuracy: 0.8275\n",
            "Epoch [10/20], Batch [1200/1500], Loss: 0.8086, Correct predictions: 2650, Total samples: 3200, Accuracy: 0.8281\n",
            "Epoch [10/20], Batch [1300/1500], Loss: 0.8196, Correct predictions: 2599, Total samples: 3200, Accuracy: 0.8122\n",
            "Epoch [10/20], Batch [1400/1500], Loss: 0.8178, Correct predictions: 2622, Total samples: 3200, Accuracy: 0.8194\n",
            "Epoch [10/20], Batch [1500/1500], Loss: 0.8066, Correct predictions: 2617, Total samples: 3200, Accuracy: 0.8178\n",
            "Epoch [11/20], Batch [100/1500], Loss: 0.8178, Correct predictions: 2618, Total samples: 3200, Accuracy: 0.8181\n",
            "Epoch [11/20], Batch [200/1500], Loss: 0.8096, Correct predictions: 2634, Total samples: 3200, Accuracy: 0.8231\n",
            "Epoch [11/20], Batch [300/1500], Loss: 0.8096, Correct predictions: 2636, Total samples: 3200, Accuracy: 0.8237\n",
            "Epoch [11/20], Batch [400/1500], Loss: 0.8018, Correct predictions: 2632, Total samples: 3200, Accuracy: 0.8225\n",
            "Epoch [11/20], Batch [500/1500], Loss: 0.7789, Correct predictions: 2678, Total samples: 3200, Accuracy: 0.8369\n",
            "Epoch [11/20], Batch [600/1500], Loss: 0.7880, Correct predictions: 2635, Total samples: 3200, Accuracy: 0.8234\n",
            "Epoch [11/20], Batch [700/1500], Loss: 0.7914, Correct predictions: 2657, Total samples: 3200, Accuracy: 0.8303\n",
            "Epoch [11/20], Batch [800/1500], Loss: 0.7806, Correct predictions: 2664, Total samples: 3200, Accuracy: 0.8325\n",
            "Epoch [11/20], Batch [900/1500], Loss: 0.7882, Correct predictions: 2643, Total samples: 3200, Accuracy: 0.8259\n",
            "Epoch [11/20], Batch [1000/1500], Loss: 0.8077, Correct predictions: 2607, Total samples: 3200, Accuracy: 0.8147\n",
            "Epoch [11/20], Batch [1100/1500], Loss: 0.7649, Correct predictions: 2681, Total samples: 3200, Accuracy: 0.8378\n",
            "Epoch [11/20], Batch [1200/1500], Loss: 0.7822, Correct predictions: 2678, Total samples: 3200, Accuracy: 0.8369\n",
            "Epoch [11/20], Batch [1300/1500], Loss: 0.7874, Correct predictions: 2659, Total samples: 3200, Accuracy: 0.8309\n",
            "Epoch [11/20], Batch [1400/1500], Loss: 0.8025, Correct predictions: 2610, Total samples: 3200, Accuracy: 0.8156\n",
            "Epoch [11/20], Batch [1500/1500], Loss: 0.7557, Correct predictions: 2676, Total samples: 3200, Accuracy: 0.8363\n",
            "Epoch [12/20], Batch [100/1500], Loss: 0.7750, Correct predictions: 2652, Total samples: 3200, Accuracy: 0.8287\n",
            "Epoch [12/20], Batch [200/1500], Loss: 0.7592, Correct predictions: 2678, Total samples: 3200, Accuracy: 0.8369\n",
            "Epoch [12/20], Batch [300/1500], Loss: 0.7455, Correct predictions: 2697, Total samples: 3200, Accuracy: 0.8428\n",
            "Epoch [12/20], Batch [400/1500], Loss: 0.7725, Correct predictions: 2649, Total samples: 3200, Accuracy: 0.8278\n",
            "Epoch [12/20], Batch [500/1500], Loss: 0.7526, Correct predictions: 2666, Total samples: 3200, Accuracy: 0.8331\n",
            "Epoch [12/20], Batch [600/1500], Loss: 0.7620, Correct predictions: 2671, Total samples: 3200, Accuracy: 0.8347\n",
            "Epoch [12/20], Batch [700/1500], Loss: 0.7386, Correct predictions: 2670, Total samples: 3200, Accuracy: 0.8344\n",
            "Epoch [12/20], Batch [800/1500], Loss: 0.7615, Correct predictions: 2686, Total samples: 3200, Accuracy: 0.8394\n",
            "Epoch [12/20], Batch [900/1500], Loss: 0.7473, Correct predictions: 2683, Total samples: 3200, Accuracy: 0.8384\n",
            "Epoch [12/20], Batch [1000/1500], Loss: 0.7415, Correct predictions: 2702, Total samples: 3200, Accuracy: 0.8444\n",
            "Epoch [12/20], Batch [1100/1500], Loss: 0.7619, Correct predictions: 2663, Total samples: 3200, Accuracy: 0.8322\n",
            "Epoch [12/20], Batch [1200/1500], Loss: 0.7580, Correct predictions: 2623, Total samples: 3200, Accuracy: 0.8197\n",
            "Epoch [12/20], Batch [1300/1500], Loss: 0.7273, Correct predictions: 2683, Total samples: 3200, Accuracy: 0.8384\n",
            "Epoch [12/20], Batch [1400/1500], Loss: 0.7483, Correct predictions: 2687, Total samples: 3200, Accuracy: 0.8397\n",
            "Epoch [12/20], Batch [1500/1500], Loss: 0.7558, Correct predictions: 2673, Total samples: 3200, Accuracy: 0.8353\n",
            "Epoch [13/20], Batch [100/1500], Loss: 0.7487, Correct predictions: 2662, Total samples: 3200, Accuracy: 0.8319\n",
            "Epoch [13/20], Batch [200/1500], Loss: 0.7341, Correct predictions: 2683, Total samples: 3200, Accuracy: 0.8384\n",
            "Epoch [13/20], Batch [300/1500], Loss: 0.7288, Correct predictions: 2720, Total samples: 3200, Accuracy: 0.8500\n",
            "Epoch [13/20], Batch [400/1500], Loss: 0.7400, Correct predictions: 2698, Total samples: 3200, Accuracy: 0.8431\n",
            "Epoch [13/20], Batch [500/1500], Loss: 0.7223, Correct predictions: 2705, Total samples: 3200, Accuracy: 0.8453\n",
            "Epoch [13/20], Batch [600/1500], Loss: 0.7279, Correct predictions: 2676, Total samples: 3200, Accuracy: 0.8363\n",
            "Epoch [13/20], Batch [700/1500], Loss: 0.7039, Correct predictions: 2713, Total samples: 3200, Accuracy: 0.8478\n",
            "Epoch [13/20], Batch [800/1500], Loss: 0.7018, Correct predictions: 2741, Total samples: 3200, Accuracy: 0.8566\n",
            "Epoch [13/20], Batch [900/1500], Loss: 0.7211, Correct predictions: 2695, Total samples: 3200, Accuracy: 0.8422\n",
            "Epoch [13/20], Batch [1000/1500], Loss: 0.7220, Correct predictions: 2690, Total samples: 3200, Accuracy: 0.8406\n",
            "Epoch [13/20], Batch [1100/1500], Loss: 0.7223, Correct predictions: 2680, Total samples: 3200, Accuracy: 0.8375\n",
            "Epoch [13/20], Batch [1200/1500], Loss: 0.7269, Correct predictions: 2687, Total samples: 3200, Accuracy: 0.8397\n",
            "Epoch [13/20], Batch [1300/1500], Loss: 0.7210, Correct predictions: 2702, Total samples: 3200, Accuracy: 0.8444\n",
            "Epoch [13/20], Batch [1400/1500], Loss: 0.7195, Correct predictions: 2702, Total samples: 3200, Accuracy: 0.8444\n",
            "Epoch [13/20], Batch [1500/1500], Loss: 0.6989, Correct predictions: 2701, Total samples: 3200, Accuracy: 0.8441\n",
            "Epoch [14/20], Batch [100/1500], Loss: 0.7264, Correct predictions: 2689, Total samples: 3200, Accuracy: 0.8403\n",
            "Epoch [14/20], Batch [200/1500], Loss: 0.7185, Correct predictions: 2679, Total samples: 3200, Accuracy: 0.8372\n",
            "Epoch [14/20], Batch [300/1500], Loss: 0.6892, Correct predictions: 2728, Total samples: 3200, Accuracy: 0.8525\n",
            "Epoch [14/20], Batch [400/1500], Loss: 0.6898, Correct predictions: 2720, Total samples: 3200, Accuracy: 0.8500\n",
            "Epoch [14/20], Batch [500/1500], Loss: 0.6987, Correct predictions: 2711, Total samples: 3200, Accuracy: 0.8472\n",
            "Epoch [14/20], Batch [600/1500], Loss: 0.6860, Correct predictions: 2717, Total samples: 3200, Accuracy: 0.8491\n",
            "Epoch [14/20], Batch [700/1500], Loss: 0.6789, Correct predictions: 2735, Total samples: 3200, Accuracy: 0.8547\n",
            "Epoch [14/20], Batch [800/1500], Loss: 0.7038, Correct predictions: 2689, Total samples: 3200, Accuracy: 0.8403\n",
            "Epoch [14/20], Batch [900/1500], Loss: 0.6936, Correct predictions: 2720, Total samples: 3200, Accuracy: 0.8500\n",
            "Epoch [14/20], Batch [1000/1500], Loss: 0.6952, Correct predictions: 2718, Total samples: 3200, Accuracy: 0.8494\n",
            "Epoch [14/20], Batch [1100/1500], Loss: 0.7001, Correct predictions: 2727, Total samples: 3200, Accuracy: 0.8522\n",
            "Epoch [14/20], Batch [1200/1500], Loss: 0.7000, Correct predictions: 2710, Total samples: 3200, Accuracy: 0.8469\n",
            "Epoch [14/20], Batch [1300/1500], Loss: 0.6687, Correct predictions: 2755, Total samples: 3200, Accuracy: 0.8609\n",
            "Epoch [14/20], Batch [1400/1500], Loss: 0.6680, Correct predictions: 2746, Total samples: 3200, Accuracy: 0.8581\n",
            "Epoch [14/20], Batch [1500/1500], Loss: 0.6770, Correct predictions: 2731, Total samples: 3200, Accuracy: 0.8534\n",
            "Epoch [15/20], Batch [100/1500], Loss: 0.6789, Correct predictions: 2735, Total samples: 3200, Accuracy: 0.8547\n",
            "Epoch [15/20], Batch [200/1500], Loss: 0.6693, Correct predictions: 2749, Total samples: 3200, Accuracy: 0.8591\n",
            "Epoch [15/20], Batch [300/1500], Loss: 0.6908, Correct predictions: 2706, Total samples: 3200, Accuracy: 0.8456\n",
            "Epoch [15/20], Batch [400/1500], Loss: 0.6862, Correct predictions: 2733, Total samples: 3200, Accuracy: 0.8541\n",
            "Epoch [15/20], Batch [500/1500], Loss: 0.6712, Correct predictions: 2743, Total samples: 3200, Accuracy: 0.8572\n",
            "Epoch [15/20], Batch [600/1500], Loss: 0.6589, Correct predictions: 2753, Total samples: 3200, Accuracy: 0.8603\n",
            "Epoch [15/20], Batch [700/1500], Loss: 0.6610, Correct predictions: 2744, Total samples: 3200, Accuracy: 0.8575\n",
            "Epoch [15/20], Batch [800/1500], Loss: 0.6504, Correct predictions: 2750, Total samples: 3200, Accuracy: 0.8594\n",
            "Epoch [15/20], Batch [900/1500], Loss: 0.6624, Correct predictions: 2735, Total samples: 3200, Accuracy: 0.8547\n",
            "Epoch [15/20], Batch [1000/1500], Loss: 0.6881, Correct predictions: 2708, Total samples: 3200, Accuracy: 0.8462\n",
            "Epoch [15/20], Batch [1100/1500], Loss: 0.6421, Correct predictions: 2756, Total samples: 3200, Accuracy: 0.8612\n",
            "Epoch [15/20], Batch [1200/1500], Loss: 0.6655, Correct predictions: 2712, Total samples: 3200, Accuracy: 0.8475\n",
            "Epoch [15/20], Batch [1300/1500], Loss: 0.6575, Correct predictions: 2761, Total samples: 3200, Accuracy: 0.8628\n",
            "Epoch [15/20], Batch [1400/1500], Loss: 0.6437, Correct predictions: 2758, Total samples: 3200, Accuracy: 0.8619\n",
            "Epoch [15/20], Batch [1500/1500], Loss: 0.6776, Correct predictions: 2691, Total samples: 3200, Accuracy: 0.8409\n",
            "Epoch [16/20], Batch [100/1500], Loss: 0.6463, Correct predictions: 2763, Total samples: 3200, Accuracy: 0.8634\n",
            "Epoch [16/20], Batch [200/1500], Loss: 0.6511, Correct predictions: 2745, Total samples: 3200, Accuracy: 0.8578\n",
            "Epoch [16/20], Batch [300/1500], Loss: 0.6435, Correct predictions: 2748, Total samples: 3200, Accuracy: 0.8588\n",
            "Epoch [16/20], Batch [400/1500], Loss: 0.6499, Correct predictions: 2747, Total samples: 3200, Accuracy: 0.8584\n",
            "Epoch [16/20], Batch [500/1500], Loss: 0.6406, Correct predictions: 2752, Total samples: 3200, Accuracy: 0.8600\n",
            "Epoch [16/20], Batch [600/1500], Loss: 0.6489, Correct predictions: 2736, Total samples: 3200, Accuracy: 0.8550\n",
            "Epoch [16/20], Batch [700/1500], Loss: 0.6408, Correct predictions: 2757, Total samples: 3200, Accuracy: 0.8616\n",
            "Epoch [16/20], Batch [800/1500], Loss: 0.6522, Correct predictions: 2754, Total samples: 3200, Accuracy: 0.8606\n",
            "Epoch [16/20], Batch [900/1500], Loss: 0.6645, Correct predictions: 2719, Total samples: 3200, Accuracy: 0.8497\n",
            "Epoch [16/20], Batch [1000/1500], Loss: 0.6435, Correct predictions: 2755, Total samples: 3200, Accuracy: 0.8609\n",
            "Epoch [16/20], Batch [1100/1500], Loss: 0.6485, Correct predictions: 2741, Total samples: 3200, Accuracy: 0.8566\n",
            "Epoch [16/20], Batch [1200/1500], Loss: 0.6313, Correct predictions: 2752, Total samples: 3200, Accuracy: 0.8600\n",
            "Epoch [16/20], Batch [1300/1500], Loss: 0.6416, Correct predictions: 2769, Total samples: 3200, Accuracy: 0.8653\n",
            "Epoch [16/20], Batch [1400/1500], Loss: 0.6355, Correct predictions: 2762, Total samples: 3200, Accuracy: 0.8631\n",
            "Epoch [16/20], Batch [1500/1500], Loss: 0.6381, Correct predictions: 2754, Total samples: 3200, Accuracy: 0.8606\n",
            "Epoch [17/20], Batch [100/1500], Loss: 0.6290, Correct predictions: 2749, Total samples: 3200, Accuracy: 0.8591\n",
            "Epoch [17/20], Batch [200/1500], Loss: 0.6357, Correct predictions: 2753, Total samples: 3200, Accuracy: 0.8603\n",
            "Epoch [17/20], Batch [300/1500], Loss: 0.6289, Correct predictions: 2754, Total samples: 3200, Accuracy: 0.8606\n",
            "Epoch [17/20], Batch [400/1500], Loss: 0.6282, Correct predictions: 2780, Total samples: 3200, Accuracy: 0.8688\n",
            "Epoch [17/20], Batch [500/1500], Loss: 0.6125, Correct predictions: 2794, Total samples: 3200, Accuracy: 0.8731\n",
            "Epoch [17/20], Batch [600/1500], Loss: 0.6260, Correct predictions: 2758, Total samples: 3200, Accuracy: 0.8619\n",
            "Epoch [17/20], Batch [700/1500], Loss: 0.6154, Correct predictions: 2773, Total samples: 3200, Accuracy: 0.8666\n",
            "Epoch [17/20], Batch [800/1500], Loss: 0.6129, Correct predictions: 2774, Total samples: 3200, Accuracy: 0.8669\n",
            "Epoch [17/20], Batch [900/1500], Loss: 0.6466, Correct predictions: 2752, Total samples: 3200, Accuracy: 0.8600\n",
            "Epoch [17/20], Batch [1000/1500], Loss: 0.6036, Correct predictions: 2781, Total samples: 3200, Accuracy: 0.8691\n",
            "Epoch [17/20], Batch [1100/1500], Loss: 0.6162, Correct predictions: 2766, Total samples: 3200, Accuracy: 0.8644\n",
            "Epoch [17/20], Batch [1200/1500], Loss: 0.6304, Correct predictions: 2770, Total samples: 3200, Accuracy: 0.8656\n",
            "Epoch [17/20], Batch [1300/1500], Loss: 0.6272, Correct predictions: 2771, Total samples: 3200, Accuracy: 0.8659\n",
            "Epoch [17/20], Batch [1400/1500], Loss: 0.6187, Correct predictions: 2782, Total samples: 3200, Accuracy: 0.8694\n",
            "Epoch [17/20], Batch [1500/1500], Loss: 0.6294, Correct predictions: 2745, Total samples: 3200, Accuracy: 0.8578\n",
            "Epoch [18/20], Batch [100/1500], Loss: 0.6126, Correct predictions: 2799, Total samples: 3200, Accuracy: 0.8747\n",
            "Epoch [18/20], Batch [200/1500], Loss: 0.6125, Correct predictions: 2778, Total samples: 3200, Accuracy: 0.8681\n",
            "Epoch [18/20], Batch [300/1500], Loss: 0.6144, Correct predictions: 2771, Total samples: 3200, Accuracy: 0.8659\n",
            "Epoch [18/20], Batch [400/1500], Loss: 0.6029, Correct predictions: 2776, Total samples: 3200, Accuracy: 0.8675\n",
            "Epoch [18/20], Batch [500/1500], Loss: 0.6131, Correct predictions: 2754, Total samples: 3200, Accuracy: 0.8606\n",
            "Epoch [18/20], Batch [600/1500], Loss: 0.6037, Correct predictions: 2785, Total samples: 3200, Accuracy: 0.8703\n",
            "Epoch [18/20], Batch [700/1500], Loss: 0.6074, Correct predictions: 2777, Total samples: 3200, Accuracy: 0.8678\n",
            "Epoch [18/20], Batch [800/1500], Loss: 0.5930, Correct predictions: 2795, Total samples: 3200, Accuracy: 0.8734\n",
            "Epoch [18/20], Batch [900/1500], Loss: 0.5973, Correct predictions: 2783, Total samples: 3200, Accuracy: 0.8697\n",
            "Epoch [18/20], Batch [1000/1500], Loss: 0.6076, Correct predictions: 2776, Total samples: 3200, Accuracy: 0.8675\n",
            "Epoch [18/20], Batch [1100/1500], Loss: 0.6097, Correct predictions: 2769, Total samples: 3200, Accuracy: 0.8653\n",
            "Epoch [18/20], Batch [1200/1500], Loss: 0.6012, Correct predictions: 2759, Total samples: 3200, Accuracy: 0.8622\n",
            "Epoch [18/20], Batch [1300/1500], Loss: 0.5947, Correct predictions: 2812, Total samples: 3200, Accuracy: 0.8788\n",
            "Epoch [18/20], Batch [1400/1500], Loss: 0.5989, Correct predictions: 2782, Total samples: 3200, Accuracy: 0.8694\n",
            "Epoch [18/20], Batch [1500/1500], Loss: 0.6077, Correct predictions: 2761, Total samples: 3200, Accuracy: 0.8628\n",
            "Epoch [19/20], Batch [100/1500], Loss: 0.5885, Correct predictions: 2808, Total samples: 3200, Accuracy: 0.8775\n",
            "Epoch [19/20], Batch [200/1500], Loss: 0.5919, Correct predictions: 2796, Total samples: 3200, Accuracy: 0.8738\n",
            "Epoch [19/20], Batch [300/1500], Loss: 0.5715, Correct predictions: 2812, Total samples: 3200, Accuracy: 0.8788\n",
            "Epoch [19/20], Batch [400/1500], Loss: 0.5967, Correct predictions: 2784, Total samples: 3200, Accuracy: 0.8700\n",
            "Epoch [19/20], Batch [500/1500], Loss: 0.6099, Correct predictions: 2763, Total samples: 3200, Accuracy: 0.8634\n",
            "Epoch [19/20], Batch [600/1500], Loss: 0.5894, Correct predictions: 2794, Total samples: 3200, Accuracy: 0.8731\n",
            "Epoch [19/20], Batch [700/1500], Loss: 0.5969, Correct predictions: 2770, Total samples: 3200, Accuracy: 0.8656\n",
            "Epoch [19/20], Batch [800/1500], Loss: 0.5799, Correct predictions: 2808, Total samples: 3200, Accuracy: 0.8775\n",
            "Epoch [19/20], Batch [900/1500], Loss: 0.5984, Correct predictions: 2780, Total samples: 3200, Accuracy: 0.8688\n",
            "Epoch [19/20], Batch [1000/1500], Loss: 0.5792, Correct predictions: 2809, Total samples: 3200, Accuracy: 0.8778\n",
            "Epoch [19/20], Batch [1100/1500], Loss: 0.6158, Correct predictions: 2734, Total samples: 3200, Accuracy: 0.8544\n",
            "Epoch [19/20], Batch [1200/1500], Loss: 0.5585, Correct predictions: 2829, Total samples: 3200, Accuracy: 0.8841\n",
            "Epoch [19/20], Batch [1300/1500], Loss: 0.5831, Correct predictions: 2772, Total samples: 3200, Accuracy: 0.8662\n",
            "Epoch [19/20], Batch [1400/1500], Loss: 0.5748, Correct predictions: 2794, Total samples: 3200, Accuracy: 0.8731\n",
            "Epoch [19/20], Batch [1500/1500], Loss: 0.5797, Correct predictions: 2783, Total samples: 3200, Accuracy: 0.8697\n",
            "Epoch [20/20], Batch [100/1500], Loss: 0.5826, Correct predictions: 2792, Total samples: 3200, Accuracy: 0.8725\n",
            "Epoch [20/20], Batch [200/1500], Loss: 0.5702, Correct predictions: 2802, Total samples: 3200, Accuracy: 0.8756\n",
            "Epoch [20/20], Batch [300/1500], Loss: 0.5840, Correct predictions: 2777, Total samples: 3200, Accuracy: 0.8678\n",
            "Epoch [20/20], Batch [400/1500], Loss: 0.5820, Correct predictions: 2780, Total samples: 3200, Accuracy: 0.8688\n",
            "Epoch [20/20], Batch [500/1500], Loss: 0.5838, Correct predictions: 2803, Total samples: 3200, Accuracy: 0.8759\n",
            "Epoch [20/20], Batch [600/1500], Loss: 0.5804, Correct predictions: 2796, Total samples: 3200, Accuracy: 0.8738\n",
            "Epoch [20/20], Batch [700/1500], Loss: 0.5587, Correct predictions: 2823, Total samples: 3200, Accuracy: 0.8822\n",
            "Epoch [20/20], Batch [800/1500], Loss: 0.5794, Correct predictions: 2749, Total samples: 3200, Accuracy: 0.8591\n",
            "Epoch [20/20], Batch [900/1500], Loss: 0.5780, Correct predictions: 2802, Total samples: 3200, Accuracy: 0.8756\n",
            "Epoch [20/20], Batch [1000/1500], Loss: 0.5825, Correct predictions: 2779, Total samples: 3200, Accuracy: 0.8684\n",
            "Epoch [20/20], Batch [1100/1500], Loss: 0.5605, Correct predictions: 2807, Total samples: 3200, Accuracy: 0.8772\n",
            "Epoch [20/20], Batch [1200/1500], Loss: 0.5653, Correct predictions: 2818, Total samples: 3200, Accuracy: 0.8806\n",
            "Epoch [20/20], Batch [1300/1500], Loss: 0.5716, Correct predictions: 2788, Total samples: 3200, Accuracy: 0.8712\n",
            "Epoch [20/20], Batch [1400/1500], Loss: 0.5526, Correct predictions: 2825, Total samples: 3200, Accuracy: 0.8828\n",
            "Epoch [20/20], Batch [1500/1500], Loss: 0.5740, Correct predictions: 2775, Total samples: 3200, Accuracy: 0.8672\n"
          ]
        }
      ],
      "source": [
        "# Data Augmentation\n",
        "transform = torchvision.transforms.Compose([\n",
        "    torchvision.transforms.RandomRotation(10),\n",
        "    torchvision.transforms.RandomAffine(degrees=0, translate=(0.1, 0.1)),\n",
        "    torchvision.transforms.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1, hue=0.1)\n",
        "])\n",
        "\n",
        "# Convert to PyTorch tensors\n",
        "x_train_tensor = torch.from_numpy(x_train).float()\n",
        "y_train_tensor = torch.from_numpy(y_train).long()\n",
        "train_dataset = TensorDataset(x_train_tensor, y_train_tensor)\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "\n",
        "# Learning Rate Scheduler (Exponential Decay)\n",
        "def lr_scheduler(epoch, lr):\n",
        "    return lr * torch.exp(torch.tensor(-0.1))\n",
        "\n",
        "# Weight Regularization (L2 Regularization)\n",
        "class CustomModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(CustomModel, self).__init__()\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.linear1 = nn.Linear(28 * 28, 128)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.batch_norm = nn.BatchNorm1d(128)\n",
        "        self.linear2 = nn.Linear(128, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.flatten(x)\n",
        "        x = self.linear1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.batch_norm(x)\n",
        "        x = self.linear2(x)\n",
        "        return x\n",
        "\n",
        "# Train the model with optimization techniques (PyTorch)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = CustomModel().to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "lr_schedule = optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=lambda epoch: lr_scheduler(epoch, 0.001))\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(20):\n",
        "    model.train()\n",
        "    total_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    for batch_idx, (inputs, labels) in enumerate(train_loader):\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        # Calculate accuracy\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "        if batch_idx % 100 == 99:  # Print every 100 batches\n",
        "            accuracy = correct / total if total != 0 else 0  # Calculate accuracy\n",
        "            print(f\"Epoch [{epoch+1}/{20}], Batch [{batch_idx+1}/{len(train_loader)}], Loss: {total_loss / 100:.4f}, Correct predictions: {correct}, Total samples: {total}, Accuracy: {accuracy:.4f}\")\n",
        "            total_loss = 0.0\n",
        "            correct = 0\n",
        "            total = 0\n",
        "\n",
        "    lr_schedule.step()\n",
        "\n",
        "# Pruning (Prune the model by removing weights below a certain threshold)\n",
        "def prune_model(model, threshold=0.1):\n",
        "    pruned_model = CustomModel()\n",
        "    pruned_model.load_state_dict(model.state_dict())\n",
        "    for name, module in pruned_model.named_modules():\n",
        "        if isinstance(module, nn.Linear):\n",
        "            module.weight.data[torch.abs(module.weight) < threshold] = 0\n",
        "    return pruned_model\n",
        "\n",
        "pruned_model = prune_model(model, threshold=0.05)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OMF5Lyco7TrK"
      },
      "source": [
        "# Результаты профилирования\n",
        "Модель, обучавшаяся в течение 20 эпох, демонстрирует последовательное увеличение точности, достигая примерно 87,8 % точности в финальной партии последней эпохи по сравнению с начальной точностью 10,0 %. Эта прогрессия указывает на обучение и улучшение модели в течение периода обучения."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f8yBLVHO-RUd",
        "outputId": "35041385-0ff1-403c-e4a3-a6b8315e5bfb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Input shape: [1, 784]\n"
          ]
        }
      ],
      "source": [
        "# Convert PyTorch model to ONNX format\n",
        "torch.onnx.export(pruned_model, torch.randn(1, 28 * 28), \"model.onnx\", export_params=True, opset_version=11)\n",
        "# Load the ONNX model\n",
        "onnx_model_path = 'model.onnx'\n",
        "onnx_model = onnx.load(onnx_model_path)\n",
        "onnx_session = onnxruntime.InferenceSession(onnx_model_path)\n",
        "\n",
        "# Get the shape of the input tensor\n",
        "input_shape = onnx_model.graph.input[0].type.tensor_type.shape.dim\n",
        "print(\"Input shape:\", [dim.dim_value for dim in input_shape])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F68nD0LCoGay",
        "outputId": "13060b7f-6912-47f3-a6e6-9bdcada417aa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Original Vectors:\n",
            "[[-0.3381818   0.87927085 -0.2973301  -0.2043918   0.6531359   0.14343423\n",
            "  -0.12295515  1.0051852  -0.46174946 -0.65287447]\n",
            " [-0.3381818   0.87927085 -0.2973301  -0.2043918   0.6531359   0.14343423\n",
            "  -0.12295515  1.0051852  -0.46174946 -0.65287447]\n",
            " [-0.3381818   0.87927085 -0.2973301  -0.2043918   0.6531359   0.14343423\n",
            "  -0.12295515  1.0051852  -0.46174946 -0.65287447]]\n",
            "\n",
            "Quantized Vectors in the extreme case:\n",
            "[[0 0 0 0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0 0 0 0]]\n",
            "\n",
            "Quantization Boundaries:\n",
            "[-0.33818179 -0.33818179 -0.33818179 -0.33818179 -0.33818179]\n"
          ]
        }
      ],
      "source": [
        "# Generate vectors using the optimized model\n",
        "def generate_vectors(model, dataset):\n",
        "    model.eval()\n",
        "    vectors = []\n",
        "    with torch.no_grad():\n",
        "        for inputs, _ in dataset:  # Unpack tuple (image, label)\n",
        "            inputs = inputs.to(device)\n",
        "            outputs = model(inputs.unsqueeze(0))  # Unsqueeze to add a batch dimension\n",
        "            vectors.append(outputs.cpu().numpy())\n",
        "    return np.concatenate(vectors)\n",
        "\n",
        "# Assuming x_test is obtained from the MNIST test dataset\n",
        "x_test = test_dataset.data.numpy()\n",
        "\n",
        "# Convert x_test to a PyTorch tensor with the appropriate dtype (float32)\n",
        "x_test_tensor = torch.tensor(x_test, dtype=torch.float32).to(device)\n",
        "\n",
        "# Flatten the input to make it 2D\n",
        "x_test_flattened = x_test_tensor.view(x_test_tensor.shape[0], -1)\n",
        "\n",
        "# Generate vectors using the optimized model\n",
        "vectors = generate_vectors(pruned_model, test_dataset)\n",
        "quantized_vectors = np.zeros_like(vectors, dtype=int)\n",
        "\n",
        "# Function to quantize a vector component\n",
        "def quantize_component(component):\n",
        "    thresholds = np.linspace(min(component), max(component), num=5)  # Define thresholds (e.g., 5 thresholds)\n",
        "    quantized_component = np.argmin(np.abs(component[:, None] - thresholds), axis=1)\n",
        "    return quantized_component, thresholds\n",
        "\n",
        "# Output quantization boundaries\n",
        "quantized_component, thresholds = quantize_component(vectors[:, 0])\n",
        "\n",
        "# Show quantized vectors and boundaries\n",
        "print(\"Original Vectors:\")\n",
        "print(vectors[:3])\n",
        "print(\"\\nQuantized Vectors in the extreme case:\")\n",
        "print(quantized_vectors[:3])\n",
        "print(\"\\nQuantization Boundaries:\")\n",
        "print(thresholds)\n",
        "\n",
        "# Reshape the input to match the expected shape [1, 784]\n",
        "x_test_reshaped = x_test_flattened[0].unsqueeze(0)  # Take the first sample and add a batch dimension"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oFLefRDT6loq"
      },
      "source": [
        "# Сравнение времени вывода с исходной моделью"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QpmdHtbv60q3"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2G-l1mZzq-jx",
        "outputId": "6bd5b991-b453-4861-96f6-6566e7676151"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Inference Time (PyTorch): 0.0012438297271728516 seconds\n",
            "Inference Time (ONNX): 0.003920793533325195 seconds\n"
          ]
        }
      ],
      "source": [
        "# Compare inference time with original model\n",
        "start_time_tf = time.time()\n",
        "predictions_tf = pruned_model(x_test_reshaped).detach().cpu().numpy()\n",
        "end_time_tf = time.time()\n",
        "inference_time_tf = end_time_tf - start_time_tf\n",
        "\n",
        "# Create a dictionary with the correct input name\n",
        "onnx_inputs = {'onnx::Flatten_0': x_test_reshaped.cpu().numpy()}\n",
        "\n",
        "start_time_onnx = time.time()\n",
        "predictions_onnx = onnx_session.run(None, onnx_inputs)\n",
        "end_time_onnx = time.time()\n",
        "inference_time_onnx = end_time_onnx - start_time_onnx\n",
        "\n",
        "print(f\"\\nInference Time (PyTorch): {inference_time_tf} seconds\")\n",
        "print(f\"Inference Time (ONNX): {inference_time_onnx} seconds\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UNGHzzCa62jR"
      },
      "source": [
        "мы видим, что onnx работает медленнее по сравнению с оригинальной моделью"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
